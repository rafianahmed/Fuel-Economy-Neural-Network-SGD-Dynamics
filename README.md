Fuel Economy Neural Network + SGD Dynamics

This project trains a Deep Neural Network to predict vehicle fuel efficiency and then explores how learning rate, batch size, and dataset size affect Stochastic Gradient Descent (SGD) using animated visualizations.

ğŸš— Dataset

The project uses the Fuel Economy dataset, preprocessed as follows:

Numerical features â†’ StandardScaler

Categorical features â†’ OneHotEncoder

Target variable (FE) â†’ Log-transformed

ğŸ— Model Architecture
Input Layer
Dense(128, ReLU)
Dense(128, ReLU)
Dense(64, ReLU)
Dense(1)    â† Regression Output


Optimizer: Adam

Loss function: MAE (Mean Absolute Error)

ğŸ“Š Training
history = model.fit(
    X, y,
    batch_size=128,
    epochs=200
)


Loss curves are plotted to visualize convergence.

ğŸ”¬ SGD Experiment

The notebook uses:

animate_sgd(
    learning_rate=0.05,
    batch_size=32,
    num_examples=256,
    steps=50,
    true_w=3.0,
    true_b=2.0
)


to show how training quality changes based on:

Learning Rate

Batch Size

Number of Examples

ğŸ§  Key Learning Outcomes

âœ” How preprocessing affects NN training
âœ” How to design and tune a regression NN
âœ” Why learning rate & batch size matter in SGD
âœ” Visual intuition of gradient descent behavior

ğŸ›  Requirements
tensorflow
numpy
pandas
matplotlib
scikit-learn
learntools

ğŸ“Œ Run Instructions
pip install -r requirements.txt
python notebook.ipynb

ğŸ“œ License

MIT License
